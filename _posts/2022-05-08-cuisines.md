---
layout: single
title:  "[머신러닝] Getting started with classification"
categories: 
  - Machine_Learning
tag: [python, blog, machinelearning]
toc: true
use_math: true
author_profile: false

---

# 지역 주제: Delicious Asian and Indian Cuisines 

---

고전적인 머신러닝의 기본 초점인 **분류**에 대해 알아보자





## 1. 분류

---

 아시아와 인도의 모든 훌륭한 요리에 대한 데이터셋에 다양한 분류 알고리즘을 사용할 것이다.



분류는 회귀 기법과 많은 공통점이 있는 지도 학습의 한 형태이다. 머신러닝이 데이터셋을 사용하여 값이나 이름을 예측하는 것이라면, 분류는 일반적으로 **이진 분류**와 **다중 클래스 분류**의 두 그룹으로 나뉜다.



- **선형 회귀 분석**을 사용하면 변수 간의 관계를 예측하고 해당 선과 관련하여 새 데이터 점이 포함될 위치를 정확하게 예측할 수 있다. 예를 들어, 9월과 12월에 호박의 가격이 얼마인지 예측할 수 있다.

- **로지스틱 회귀 분석**을 통해 "이진 범주"를 발견할 수 있다. 예를 들어, 이 가격대에서 호박은 주황색인가, 아닌가?를 알 수 있다.

  

분류는 다양한 알고리즘을 사용하여 데이터 점의 레이블이나 클래스를 결정하는 다른 방법을 결정한다. 이 요리 데이터를 사용하여 **재료 그룹**을 관찰함으로써 **음식의 유래**를 결정할 수 있는지 알아보자.



분류는 머신러닝 연구자와 데이터 과학자의 기본 활동 중 하나이다. *"이 이메일은 스팸인지 아닌지"를 분류*하는 것과 같은 이진 값의 기본 분류에서 컴퓨터 비전을 사용한 복잡한 이미지 분류 및 분할에 이르기까지 데이터를 클래스로 정렬하고 그에 대한 질문을 할 수 있는 것은 항상 유용하다.

보다 과학적인 방법으로 프로세스를 설명하기 위해, 분류 방법에서는 *입력 변수와 출력 변수 사이의 관계를 매핑*할 수 있는 예측 모델을 만든다.

데이터를 정리하고 시각화하고 머신러닝 작업에 대비하는 프로세스를 시작하기 전에, 머신러닝을 활용하여 데이터를 분류하는 다양한 방법에 대해 알아보자.

- 통계에서 파생된 고전적인 머신러닝을 사용한 분류는 `smoker`,  `weight`,  `age`와 같은 특성을 사용하여 ***X*질환의 발병 가능성**을 결정한다. 
- 회귀와 유사한 **지도 학습** 기법으로, 데이터에 레이블이 지정되고 머신러닝 알고리즘은 이러한 레이블을 사용하여 데이터셋의 클래스(또는 '특성')를 분류하고 예측하여 그룹 또는 결과에 할당한다.

✅ 요리에 대한 데이터 세트를 생각해보자. 

- 다중 클래스 모델이 대답할 수 있는 것은 무엇일까? 
- 이진 모델은 무엇을 답할 수 있을까? 
- 주어진 요리가 페누그릭을 사용할 가능성이 있는지 여부를 결정하기를 원한다면? 
- 스타 아니스, 아티초크, 콜리플라워, 그리고 고추냉이가 가득한 식료품 봉지를 선물로 받는다면, 전형적인 인도 요리를 만들 수 있을까?





### Classifier

---

우리가 이 요리 데이터셋에 대해 묻고 싶은 것은  몇 가지 잠재적인 국가 요리를 대상으로 하기 때문에 사실 **다중 클래스** 문제이다. 성분 배치가 주어졌을 때, 이 많은 클래스 중 어떤 데이터가 적합할까?

`Scikit-learn`은 해결하려는 문제의 종류에 따라 **데이터를 분류**하는 데 사용할 수 있는 몇 가지 다른 알고리즘을 제공한다. 이러한 알고리즘 중 몇 가지에 대해 공부해보자.





### 데이터 구하기

---

이 프로젝트를 시작하기 전에 가장 먼저 해야 할 일은 **데이터를 정제**하고 **균형**을 맞추어 더 나은 결과를 얻는 것이다.

가장 먼저 설치해야 할 것은 `imblearn`이다. 이것은 데이터의 균형을 더 잘 조정할 수 있도록 지원하는 Scikit-learn 패키지이다.



1. `imblearn`을 설치하기 위해 다음과 같이 `pip install`을 실행한다.

   ```python
   pip install imblearn
   ```

   

2. 데이터를 가져오고 시각화하는 데 필요한 패키지와 `imblearn`의 `SMOTE`를 불러온다.

   ```python
   import pandas as pd
   import matplotlib.pyplot as plt
   import matplotlib as mpl
   import numpy as np
   from imblearn.over_sampling import SMOTE
   ```

   이제 다음에 불러온 데이터를 읽도록 설정되었다.

   

3. 다음 작업은 데이터를 불러오는 것이다.

   ```python
   datapath = "https://raw.githubusercontent.com/codingalzi/ML-For-Beginners/main/4-Classification/data/"
   df  = pd.read_csv(datapath+"cuisines.csv")
   ```

   `read_csv`를 이용하여 csv파일인 *cuisines.csv*의 내용을 읽고 변수 `df`에 할당한다.

   

4. 데이터의 모양을 확인해보자.

   ```python
   df.head()	# 첫 5행 출력
   ```

   |      | Unnamed: 0 | cuisine | almond | angelica | anise | anise_seed | apple | apple_brandy | apricot | armagnac | ...  | whiskey | white_bread | white_wine | whole_grain_wheat_flour | wine | wood | yam  | yeast | yogurt | zucchini |
   | ---- | ---------- | ------- | ------ | -------- | ----- | ---------- | ----- | ------------ | ------- | -------- | ---- | ------- | ----------- | ---------- | ----------------------- | ---- | ---- | ---- | ----- | ------ | -------- |
   | 0    | 65         | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ...  | 0       | 0           | 0          | 0                       | 0    | 0    | 0    | 0     | 0      | 0        |
   | 1    | 66         | indian  | 1      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ...  | 0       | 0           | 0          | 0                       | 0    | 0    | 0    | 0     | 0      | 0        |
   | 2    | 67         | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ...  | 0       | 0           | 0          | 0                       | 0    | 0    | 0    | 0     | 0      | 0        |
   | 3    | 68         | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ...  | 0       | 0           | 0          | 0                       | 0    | 0    | 0    | 0     | 0      | 0        |
   | 4    | 69         | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ...  | 0       | 0           | 0          | 0                       | 0    | 0    | 0    | 0     | 1      | 0        |

    

5. `info()`를 호출하여 이 데이터의 정보를 알아보자.

   ```python
   df.info()
   ```

   ```output
   <class 'pandas.core.frame.DataFrame'>
   RangeIndex: 2448 entries, 0 to 2447
   Columns: 385 entries, Unnamed: 0 to zucchini
   dtypes: int64(384), object(1)
   memory usage: 7.2+ MB	
   ```

   
   
   





### 데이터 탐색 - cuisines

---

각 요리의 데이터의 분포를 알아보자.



1. `barsh()`를 호출하여 데이터를 바 형태로 시각화한다.

   ```python
   df.cuisine.value_counts().plot.barh()
   ```

   ![cuisine data distribution](https://github.com/codingalzi/ML-For-Beginners/raw/main/4-Classification/1-Introduction/images/cuisine-dist.png)

   한정된 수의 요리가 있지만, 데이터의 분포가 고르지 않다. 이를 고치기 전에 조금 더 살펴보자.

   

2. 각 요리가 얼마나 많은 데이터를 사용할 수 있는지 알아보고 출력한다.

   ```python
   thai_df = df[(df.cuisine == "thai")]
   japanese_df = df[(df.cuisine == "japanese")]
   chinese_df = df[(df.cuisine == "chinese")]
   indian_df = df[(df.cuisine == "indian")]
   korean_df = df[(df.cuisine == "korean")]
   
   print(f'thai df: {thai_df.shape}')
   print(f'japanese df: {japanese_df.shape}')
   print(f'chinese df: {chinese_df.shape}')
   print(f'indian df: {indian_df.shape}')
   print(f'korean df: {korean_df.shape}')
   ```

   ```output
   thai df: (289, 385)
   japanese df: (320, 385)
   chinese df: (442, 385)
   indian df: (598, 385)
   korean df: (799, 385) 
   ```

   
   





### 데이터 정제 및 시각화 - 식재료 알아보기

---

이제 데이터를 더 깊이 파고들어 각 요리의 대표적인 식재료가 무엇인지 알아보자. 우선 요리를 구분하는 데 혼란을 일으키는 **반복적인 데이터를 제거**해야 하는데, 이 문제에 대해 알아보도록 하자.



1. 파이썬에서 `create_ingredient()` 함수를 만들어 식재료 데이터 프레임을 만든다. 이 함수는 도움이 되지 않는 열을 제거하고 성분을 개수에 따라 정렬한다.

   ```python
   def create_ingredient_df(df):
       ingredient_df = df.T.drop(['cuisine','Unnamed: 0']).sum(axis=1).to_frame('value')	# 필요 없는 열 제거
       ingredient_df = ingredient_df[(ingredient_df.T != 0).any()]
       ingredient_df = ingredient_df.sort_values(by='value', ascending=False,
       inplace=False)
       return ingredient_df
   ```

   이제 이 함수를 사용하여 요리별로 가장 인기 있는 10대 식재료에 대한 아이디어를 얻을 수 있다.

   

2. `create_ingredient()`를 호출하고 `barch()`를 호출하여 시각화한다.

   ```python
   # 태국 음식의 10대 식재료
   thai_ingredient_df = create_ingredient_df(thai_df)
   thai_ingredient_df.head(10).plot.barh()
   ```

   ![thai](https://github.com/codingalzi/ML-For-Beginners/raw/main/4-Classification/1-Introduction/images/thai.png)

   

   ```python
   # 일본 음식의 10대 식재료
   japanese_ingredient_df = create_ingredient_df(japanese_df)
   japanese_ingredient_df.head(10).plot.barh()
   ```

   ![japanese](https://github.com/codingalzi/ML-For-Beginners/raw/main/4-Classification/1-Introduction/images/japanese.png)

   

   ```python
   # 중국 음식의 10대 식재료
   chinese_ingredient_df = create_ingredient_df(chinese_df)
   chinese_ingredient_df.head(10).plot.barh()
   ```

   ![chinese](https://github.com/codingalzi/ML-For-Beginners/raw/main/4-Classification/1-Introduction/images/chinese.png)

   

   ```python
   # 인도 음식의 10대 식재료
   indian_ingredient_df = create_ingredient_df(indian_df)
   indian_ingredient_df.head(10).plot.barh()
   ```

   ![indian](https://github.com/codingalzi/ML-For-Beginners/raw/main/4-Classification/1-Introduction/images/indian.png)

   

   ```python
   # 한국 음식의 10대 식재료
   korean_ingredient_df = create_ingredient_df(korean_df)
   korean_ingredient_df.head(10).plot.barh()
   ```

   ![korean](https://github.com/codingalzi/ML-For-Beginners/raw/main/4-Classification/1-Introduction/images/korean.png)

   

3. `drop()`을 호출하여 요리들을 구분하는데 혼란을 일으키는 가장 흔한 재료들(쌀, 마늘, 생강)을 제거한다.

   ```python
   feature_df= df.drop(['cuisine','Unnamed: 0','rice','garlic','ginger'], axis=1)
   labels_df = df.cuisine #.unique()
   feature_df.head()
   ```

   | index | almond | angelica | anise | anise\_seed | apple | apple\_brandy | apricot | armagnac | artemisia | artichoke | asparagus | avocado | bacon | baked\_potato | balm | banana | barley | bartlett\_pear | basil | bay  |
   | :---: | ------ | -------- | ----- | ----------- | ----- | ------------- | ------- | -------- | --------- | --------- | --------- | ------- | ----- | ------------- | ---- | ------ | ------ | -------------- | ----- | ---- |
   |   0   | 0      | 0        | 0     | 0           | 0     | 0             | 0       | 0        | 0         | 0         | 0         | 0       | 0     | 0             | 0    | 0      | 0      | 0              | 0     | 0    |
   |   1   | 1      | 0        | 0     | 0           | 0     | 0             | 0       | 0        | 0         | 0         | 0         | 0       | 0     | 0             | 0    | 0      | 0      | 0              | 0     | 0    |
   |   2   | 0      | 0        | 0     | 0           | 0     | 0             | 0       | 0        | 0         | 0         | 0         | 0       | 0     | 0             | 0    | 0      | 0      | 0              | 0     | 0    |
   |   3   | 0      | 0        | 0     | 0           | 0     | 0             | 0       | 0        | 0         | 0         | 0         | 0       | 0     | 0             | 0    | 0      | 0      | 0              | 0     | 0    |
   |   4   | 0      | 0        | 0     | 0           | 0     | 0             | 0       | 0        | 0         | 0         | 0         | 0       | 0     | 0             | 0    | 0      | 0      | 0              | 0     | 0    |





### 데이터셋 균형 맞추기

---

데이터를 정제했으니 이제 SMOTE (Synthetic Minority Over-sampling Techniqu)를 사용하여 균형을 맞춘다.



1. `fit_resample()`을 호출한다. 이것은 새로운 샘플을 생성하여 입력한다.

   ```python
   oversample = SMOTE()
   transformed_feature_df, transformed_label_df = oversample.fit_resample(feature_df, labels_df)
   ```

   데이터의 균형을 유지하면서 데이터를 분류할 때 더 나은 결과를 얻을 수 있다. 

   **이진 분류**에 대해 생각해 보자. 대부분의 데이터가 하나의 클래스인 경우, 단지 그 클래스에 더 많은 데이터가 있기 때문에 머신러닝 모델은 해당 클래스를 *더 자주 예측* 한다. 데이터의 균형을 맞추려면 **편향된 데이터**가 필요하며 이것은 이러한 불균형을 제거하는 데 도움이 된다.

   

2. 각 식재료의 레이블 수를 확인해 보자.

   ```python
   print(f'new label count: {transformed_label_df.value_counts()}')
   print(f'old label count: {df.cuisine.value_counts()}')
   ```

   ```output
   new label count: korean      799
   chinese     799
   indian      799
   japanese    799
   thai        799
   Name: cuisine, dtype: int64
   old label count: korean      799
   indian      598
   chinese     442
   japanese    320
   thai        289
   Name: cuisine, dtype: int64
   ```

   데이터가 잘 정제되었고 균형이 맞는 것을 확인할 수 있다.

   

3. 마지막 단계는 레이블과 특성을 포함하는 균형 잡힌 데이터를 파일로 내보낼 수 있는 새로운 데이터 프레임에 저장하는 것이다.

   ```python
   transformed_df = pd.concat([transformed_label_df,transformed_feature_df],axis=1, join='outer')
   ```

   

4. `transformed_df.head()`와 `transformed_df.info()`를 사용하여 데이터를 한번 더 확인할 수 있다. 다음에 사용하기 위해 이 데이터의 복사본을 저장할 수 있다.

   ```python
   transformed_df.head()
   transformed_df.info()
   transformed_df.to_csv(datapath + "cleaned_cuisines.csv")	# 파일을 저장할 경로 지정
   ```

   



## 2. 요리 분류기 1

---

데이터셋을 다양한 분류기를 사용하여 **식재료 그룹을 기반으로 특정 국가 음식을 예측**할 수 있다. 분류 작업에 알고리즘을 활용할 수 있는 몇 가지 방법에 대해 자세히 알아보자.





### 한 국가 요리 예측

---

1. 데이터 불러오기

   ```python
   cuisines_df = pd.read_csv(datapath+"cleaned_cuisines.csv")
   	# transformed_df
   cuisines_df.head()
   ```

|      | Unnamed: 0 | cuisine | almond | angelica | anise | anise_seed | apple | apple_brandy | apricot | armagnac | ...  | whiskey | white_bread | hite_wine | whole_grain_wheat_flour | wine | wood | yam  | yeast | yogurt | zucchini |
| ---- | ---------- | ------- | ------ | -------- | ----- | ---------- | ----- | ------------ | ------- | -------- | ---- | ------- | ----------- | --------- | ----------------------- | ---- | ---- | ---- | ----- | ------ | -------- |
| 0    | 0          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ...  | 0       | 0           | 0         | 0                       | 0    | 0    | 0    | 0     | 0      | 0        |
| 1    | 1          | indian  | 1      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ...  | 0       | 0           | 0         | 0                       | 0    | 0    | 0    | 0     | 0      | 0        |
| 2    | 2          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ...  | 0       | 0           | 0         | 0                       | 0    | 0    | 0    | 0     | 0      | 0        |
| 3    | 3          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ...  | 0       | 0           | 0         | 0                       | 0    | 0    | 0    | 0     | 0      | 0        |
| 4    | 4          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ...  | 0       | 0           | 0         | 0                       | 0    | 0    | 0    | 0     | 1      | 0        |

   

2. 훈련을 위해 X 와 y를 두 개의 데이터 프레임으로 나눈다. `cuisine`은 레이블 데이터 프레임이 될 수 있다.

   ```python
   cuisines_label_df = cuisines_df['cuisine']
   cuisines_label_df.head()
   ```

   ```output
   0    indian
   1    indian
   2    indian
   3    indian
   4    indian
   Name: cuisine, dtype: object
   ```

   

3. `drop()`을 호출하여 `Unnamed: 0` 열과 `cuisine`열을 제거하고 나머지 데이터를 훈련 가능한 특성으로 저장한다.  

   ```python
   cuisines_feature_df = cuisines_df.drop(['Unnamed: 0', 'cuisine'], axis=1)
   cuisines_feature_df.head()
   ```

   |      | almond | angelica | anise | anise_seed | apple | apple_brandy | apricot | armagnac | ...  | whiskey | white_bread | hite_wine | whole_grain_wheat_flour | wine | wood | yam  | yeast | yogurt | zucchini |
   | ---- | ------ | -------- | ----- | ---------- | ----- | ------------ | ------- | -------- | ---- | ------- | ----------- | --------- | ----------------------- | ---- | ---- | ---- | ----- | ------ | -------- |
   | 0    | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ...  | 0       | 0           | 0         | 0                       | 0    | 0    | 0    | 0     | 0      | 0        |
   | 1    | 1      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ...  | 0       | 0           | 0         | 0                       | 0    | 0    | 0    | 0     | 0      | 0        |
   | 2    | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ...  | 0       | 0           | 0         | 0                       | 0    | 0    | 0    | 0     | 0      | 0        |
   | 3    | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ...  | 0       | 0           | 0         | 0                       | 0    | 0    | 0    | 0     | 0      | 0        |
   | 4    | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ...  | 0       | 0           | 0         | 0                       | 0    | 0    | 0    | 0     | 1      | 0        |





### 분류기 선택

---

이제 데이터가 정제되었고 훈련할 준비가 되었으므로 작업에 사용할 알고리즘을 결정해야 한다.

Scikit-learn은 분류를 **지도학습**으로 분류하며, 이 카테고리에서 분류할 수 있는 여러 가지 방법을 찾을 수 있다. 

🎈 분류 기법을 포함하는 방법들

- 선형 모델(Linear Models)
- 서포트 벡터 머신(Support Vector Machines, SVM)
- 확률적 경사 하강법(Stochastic Gradient Descent)
- k-최근접 이웃(kNN)
- 가우시안 프로세스
- 결정 트리
- 앙상블 기법
- 다중 클래스 및 다중 출력 알고리즘(다중 클래스 및 다중 레이블 분류, 다중 클래스 다중 출력 분류)



> 지도 학습 참고: https://scikit-learn.org/stable/supervised_learning.html





#### 분류기 선택 방법

---

보통 *여러 개를 실행* 해보고 좋은 결과를 찾는 것이 테스트하는 방법이다. `Scikit-learn`은 생성된 데이터셋에 대한  *[side-by-side comparison](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html)*을 제공하는데, 다음과 같은 분류기들을 **비교**하고 결과를 **시각화**하여 보여준다.

- KNeighbors
- Linear SVM, RBF SVM
- GaussianProcessClassifier
- DecisionTreeClassifier
- RandomForestClassifier
- MLPClassifier
- AdaBoostClassifier
- GaussianNB
- QuadraticDiscrinationAnalysis

<img src="https://github.com/codingalzi/ML-For-Beginners/raw/main/4-Classification/2-Classifiers-1/images/comparison.png" alt="comparison of classifiers"  />



그러나 마구 추측하는 것보다 더 나은 방법은 다운로드 가능한 [머신러닝 Cheat sheet](https://docs.microsoft.com/azure/machine-learning/algorithm-cheat-sheet?WT.mc_id=academic-15963-cxa)의 아이디어를 따르는 것이다. 여기서,  다중 클래스 문제에 대해 몇 가지 선택사항이 있다는 것을 알 수 있다.

<img src="https://github.com/codingalzi/ML-For-Beginners/raw/main/4-Classification/2-Classifiers-1/images/cheatsheet.png" alt="cheatsheet for multiclass problems" style="zoom: 67%;" />

> Microsoft의 알고리즘 치트 시트의 한 부분(다중 클래스 분류 옵션 상세 내역)





#### 추리

---

우리가 가지고 있는 제약 조건들을 고려할 때, 다른 *접근 방식* 들을 통해 방법을 추론할 수 있는지 알아보자.

- **신경망은 너무 무겁다.** 정제되었지만 아주 작은 데이터 셋과, 노트북을 통해 로컬로 훈련을 실행하고 있다는 사실을 감안할 때 신경망은 이 작업에 너무 무겁다.
- **2-클래스 분류기가 없다.** 우리는 2-클래스 분류기를 사용하지 않기 때문에, One-vs-All 방식을 배제한다.
- **결정 트리 또는 로지스틱 회귀가 작동할 수 있다.** 결정 트리가 작동하거나 다중 클래스 데이터에 대해 로지스틱 회귀를 수행할 수 있다.
- **다중 클래스 향상된 결정 트리(Multiclass Boosted Decision Trees)는 다른 문제를 해결한다.** 다중 클래스 향상된 결정 트리는 예를 들어 순위를 구축하도록 설계된 작업과 같은 비파라미터 작업에 가장 적합하므로 유용하지 않다.





#### Scikit-learn 사용하기

---

우리는 `Scikit-learn`을 사용하여 데이터를 분석할 것이다. 



Scikit-learn에서는 **로지스틱 회귀**를  사용하는 여러 가지 방법이 있다. [전달할 파라미터](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logistic regressio#sklearn.linear_model.LogisticRegression)를 살펴보자.

✨ 로지스틱 회귀를 수행하도록 요청할 때 지정해야 하는 중요 파라미터

- `multi_class` : 특정 동작을 적용
- `solver` : 사용할 알고리즘
- 모든 `solver`를 모든 `multi_class` 값과 쌍으로 구성할 수 있는 것은 아니다.



문서에 따르면, 다중 클래스 사례에서 훈련 알고리즘은 다음과 같다:

- `multi_class` 옵션이 `ovr`로 설정된 경우, **one-vs-rest(OvR)** scheme을 사용한다.
- `multi_class` 옵션이 `multinomial`로 설정된 경우, **교차 엔트로피 손실(cross-entropy loss)**을 사용한다. (현재 다항식 옵션은 'lbfgs', 'sag', 'saga' 및 'newton-cg' solver에서만 지원된다.)

> 🎓 여기서 scheme은 'ovr'(one-vs-rest) 또는 '다항식'일 수 있다. 로지스틱 회귀는 실제로 이진 분류를 지원하도록 설계되었기 때문에, 이러한 방식을 통해 다중 클래스 분류 작업을 더 잘 처리할 수 있다.

> 🎓 'solver'는 "최적화 문제에 사용할 알고리즘"으로 정의된다.

`Scikit-learn`은 `solver`들이 다양한 종류의 데이터 구조에서 나타나는 다양한 문제를 처리하는 방법을 설명하는 이 표를 제공한다.

<img src="https://raw.githubusercontent.com/codingalzi/ML-For-Beginners/main/4-Classification/2-Classifiers-1/images/solvers.png" alt="img" style="zoom:50%;" />





### 데이터 분리

---

훈련 시험을 위해 로지스틱 회귀에 초점을 맞출 수 있다. `train_test_split()`을 호출하여 데이터를 훈련 및 테스트 그룹으로 분리한다.

```python
X_train, X_test, y_train, y_test = train_test_split(cuisines_feature_df, cuisines_label_df, test_size=0.3)
```





### 로지스틱 회귀 적용

---

다중 클래스 사례를 사용 중이므로 사용할 *scheme*과 설정할 *solver*를 선택해야 한다. 다중 클래스 설정 및 **liblinear** solver와 함께 로지스틱 회귀를 사용하여 훈련한다.

1. multi_class를 `ovr`로 설정하고 solver를 `liblinear`로 설정한 로지스틱 회귀를 생성한다.

   ```python
   lr = LogisticRegression(multi_class='ovr',solver='liblinear')
   model = lr.fit(X_train, np.ravel(y_train))
   
   accuracy = model.score(X_test, y_test)
   print ("Accuracy is {}".format(accuracy))
   ```

   ```output
   Accuracy is 0.8015012510425354
   ```

   정확도가 80%가 넘는 것을 확인할 수 있다.

   

   ✅ 기본으로 자주 설정되는 `lbfgs`같은 다른 solver도 시도해보자.

   > Pandas [`ravel`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.ravel.html) 함수를 사용하면 필요할 때 데이터를 평평하게 만든다.

   

2. 하나의 데이터 행(#50)을 테스트하면 이 모델이 작동하는 것을 볼 수 있다.

   ```python
   print(f'ingredients: {X_test.iloc[50][X_test.iloc[50]!=0].keys()}')
   print(f'cuisine: {y_test.iloc[50]}')
   ```

   ```output
   ingredients: Index(['cilantro', 'onion', 'potato', 'tomato'], dtype='object')
   cuisine: indian
   ```

   

3. 더 깊이 들어가서, 이 예측의 정확도를 확인할 수 있다.

   ```python
   test= X_test.iloc[50].values.reshape(-1, 1).T
   proba = model.predict_proba(test)
   classes = model.classes_
   resultdf = pd.DataFrame(data=proba, columns=classes)
   
   topPrediction = resultdf.T.sort_values(by=[0], ascending = [False])
   topPrediction.head()
   ```

   | index    | 0                     |
   | -------- | :---------|
   | indian   | 0\.753535 |
   | chinese  | 0\.164452 |
   | japanese | 0\.039409 |
   | thai     | 0\.023056 |
   | korean   | 0\.019548 |
   
   
   
4. 분류 보고서를 출력하여 더 자세히 알아본다.

   ```python
   y_pred = model.predict(X_test)
   print(classification_report(y_test,y_pred))
   ```

   ```output
                 precision    recall  f1-score   support
   
        chinese       0.71      0.69      0.70       243
         indian       0.93      0.91      0.92       230
       japanese       0.79      0.76      0.77       248
         korean       0.80      0.80      0.80       249
           thai       0.80      0.86      0.83       229
   
       accuracy                           0.80      1199
      macro avg       0.80      0.80      0.80      1199
   weighted avg       0.80      0.80      0.80      1199
   ```
   
   



## 3. 요리 분류기 2

---

**숫자 데이터를 분류**하는 더 많은 방법과 한 분류기와 다른 분류기를 선택하는 것이  미치는 영향에 대해서 알아보자. 



### 분류 지도

---

위에서 Microsoft의 치트 시트를 사용하여 데이터를 분류할 때 사용할 수 있는 다양한 옵션에 대해 알아보았다. `Scikit-learn`은 *추정기(분류기의 다른 용어)를 더 좁히는 데 도움* 이 될 수 있는 유사하지만 보다 세분화된 치트 시트를 제공한다.

<img src="https://github.com/codingalzi/ML-For-Beginners/raw/main/4-Classification/3-Classifiers-2/images/map.png" alt="ML Map from Scikit-learn" style="zoom:50%;" />

> Tip: [map online](https://scikit-learn.org/stable/tutorial/machine_learning_map/)에 접속하고 경로를 클릭하면 해당 문서를 읽을 수 있다.





### 계획

---

이 지도는 데이터를 명확하게 파악하면 다음과 같이 결정에 이르는 경로를 *따라갈* 수 있으므로 매우 유용하다.

📌

- **50개 이상**의 샘플을 가지고 있다.
- **한 범주**를 예측하고 싶다.
- **레이블이 지정**된 데이터가 있다.
- **10만개 미만**의 샘플을 가지고 있다.
- ✨ **선형 SVC**를 선택할 수 있다.
- 만약 안 된다면, 수치 데이터를 가지고 있기 때문에
  - ✨ **KNeighbors** 분류기를 사용해 볼 수 있다.
    - 그래도 문제가 해결되지 않으면, **SVC 및 앙상블 분류기**를 사용해 본다.







### 데이터 분리

---



1. 필요한 라이브러리 가져오기:

   ```python
   from sklearn.neighbors import KNeighborsClassifier
   from sklearn.linear_model import LogisticRegression
   from sklearn.svm import SVC
   from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
   from sklearn.model_selection import train_test_split, cross_val_score
   from sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report, precision_recall_curve
   import numpy as npxxxxxxxxxx from sklearn.neighbors import KNeighborsClassifierfrom sklearn.linear_model import LogisticRegressionfrom sklearn.svm import SVCfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifierfrom sklearn.model_selection import train_test_split, cross_val_scorefrom sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report, precision_recall_curveimport numpy as npfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
   ```

    

2. 훈련 및 테스트 데이터 분리하기

   ```python
   X_train, X_test, y_train, y_test = train_test_split(cuisines_feature_df, cuisines_label_df, test_size=0.3)
   ```

    
   
   

### 선형 SVC 분류기

---

Support-Vector clustering(SVC)는 머신러닝 기술인 *Support-Vector machine* 의 하위 계열이다. 이 방법에서는 'kernel'을 선택하여 레이블을 **군집화**하는 방법을 결정할 수 있다. 



📌 선형 SVC 분류기 파라미터

- 'C' 파라미터: 파라미터의 영향을 조절하는 '규제'를 의미
- 커널:  [여러 가지](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) 중 하나일 수 있다. 여기서는 **선형 SVC**를 활용하도록 `'linear'`으로 설정
- Probability: 기본적으로 'false'로 설정되지만, 여기서는 **확률 추정치를 수집**하기 위해 `'true'`로 설정
- random state: 확률값을 얻기 위해 데이터를 섞어야 하는데, 이를 위해 '0'으로 설정





#### 선형 SVC 적용

---

분류기 배열을 만드는 것으로 시작하며, 테스트하는 대로 이 어레이에 점진적으로 추가한다.



1. Linear SVC로 시작하기

   ```python
   C = 10
   # Create different classifiers.
   classifiers = {
       'Linear SVC': SVC(kernel='linear', C=C, probability=True,random_state=0)
   }
   ```

    

2. 모델을 Linear SVC로 훈련하고 보고서 출력

   ```python
   n_classifiers = len(classifiers)
   
   for index, (name, classifier) in enumerate(classifiers.items()):
       classifier.fit(X_train, np.ravel(y_train))
   
       y_pred = classifier.predict(X_test)
       accuracy = accuracy_score(y_test, y_pred)
       print("Accuracy (train) for %s: %0.1f%% " % (name, accuracy * 100))
       print(classification_report(y_test,y_pred))
   ```

   ```output
   Accuracy (train) for Linear SVC: 80.5% 
                 precision    recall  f1-score   support
   
        chinese       0.72      0.73      0.73       257
         indian       0.87      0.88      0.88       224
       japanese       0.82      0.77      0.79       228
         korean       0.85      0.77      0.81       253
           thai       0.78      0.88      0.82       237
   
       accuracy                           0.80      1199
      macro avg       0.81      0.81      0.81      1199
   weighted avg       0.81      0.80      0.80      1199
   
   ```
   
   정확도 80.5%로 결과가 꽤 좋다.





### K-Neighbors 분류기

---

K-Neighbors는 머신러닝 방법의 *"neighbors"* 계열의 일부로, **지도 학습**과 **비지도 학습** 모두에 사용할 수 있다. 이 방법에서는 *데이터에 대한 일반화된 레이블을 예측* 할 수 있도록 미리 정의된 개수의 점이 생성되고 이러한 점 주위에 데이터가 모인다.



#### K-Neighbors 분류기 적용

---

이전의 분류기는 좋았고, 데이터에도 잘 작동했지만, 아마도 더 나은 정확도를 얻을 수 있을 것이다. K-Neighbors 분류기를 사용해 보자.



1. 분류기 배열에서 Linear SVC 항목 뒤에 쉼표를 추가하고  `'KNN classifier': KNeighborsClassifier(C)`줄을 추가

   ```python
   classifiers = {
       'Linear SVC': SVC(kernel='linear', C=C, probability=True,random_state=0),
       'KNN classifier': KNeighborsClassifier(C),
   }
   ```

   ```output
   ~~~ Linear SVC 결과 생략 ~~~
   
   Accuracy (train) for KNN classifier: 73.9% 
                 precision    recall  f1-score   support
   
        chinese       0.72      0.69      0.70       257
         indian       0.84      0.81      0.83       224
       japanese       0.63      0.83      0.72       228
         korean       0.91      0.58      0.71       253
           thai       0.69      0.80      0.74       237
   
       accuracy                           0.74      1199
      macro avg       0.76      0.74      0.74      1199
   weighted avg       0.76      0.74      0.74      1199
   ```

   선형 SVC 분류기보다 더 나쁜 결과를 보인다.

   

   ✅ [K-Neighbors](https://scikit-learn.org/stable/modules/neighbors.html#neighbors) 알아보기





### Support Vector 분류기

---

Support-Vector 분류기는 분류 및 회귀 작업에 사용되는 머신러닝 방법의 *Support-Vector Machine* 계열의 일부이다. SVM은 두 범주 간의 *거리를 최대화* 하기 위해 **공간 내 지점에 훈련 예제를 매핑**한다. 후속 데이터는 해당 범주를 예측할 수 있도록 이 공간에 매핑된다.



#### Support Vector 분류기 적용

---

Support Vector 분류기를 사용하여 더 나은 정확도를 위해 노력해 보자.



1. K-Neighbors 항목 뒤에 쉼표를 추가하고 `'SVC': SVC(),`를 추가

   ```python
   classifiers = {
       'Linear SVC': SVC(kernel='linear', C=C, probability=True,random_state=0),
       'KNN classifier': KNeighborsClassifier(C),
       'SVC': SVC(),
   }
   ```

   ```output
   ~~~ Linear SVC, KNN 결과 생략 ~~~
   
   Accuracy (train) for SVC: 84.7% 
                 precision    recall  f1-score   support
   
        chinese       0.76      0.79      0.77       257
         indian       0.92      0.92      0.92       224
       japanese       0.88      0.81      0.84       228
         korean       0.89      0.83      0.86       253
           thai       0.81      0.90      0.85       237
   
       accuracy                           0.85      1199
      macro avg       0.85      0.85      0.85      1199
   weighted avg       0.85      0.85      0.85      1199
   
   ```

   정확도 84.7%로 지금까지의 분류기 중 가장 좋은 결과를 보인다.

   

   ✅ [Support-Vectors](https://scikit-learn.org/stable/modules/svm.html#svm) 알아보기





### 앙상블 분류기

---

지난번 테스트는 꽤 좋았지만 끝까지 해보자. 앙상블 분류기, 특히 랜덤 포레스트와 AdaBoost를 사용해 보자.



```python
classifiers = {
    'Linear SVC': SVC(kernel='linear', C=C, probability=True,random_state=0),
    'KNN classifier': KNeighborsClassifier(C),
    'SVC': SVC(),
    'RFST': RandomForestClassifier(n_estimators=100),
    'ADA': AdaBoostClassifier(n_estimators=100)
}
```

```output
~~~ Linear SVC, KNN,SVC  결과 생략 ~~~

Accuracy (train) for RFST: 84.6% 
              precision    recall  f1-score   support

     chinese       0.79      0.84      0.81       257
      indian       0.89      0.91      0.90       224
    japanese       0.85      0.80      0.83       228
      korean       0.89      0.81      0.85       253
        thai       0.81      0.89      0.85       237

    accuracy                           0.85      1199
   macro avg       0.85      0.85      0.85      1199
weighted avg       0.85      0.85      0.85      1199

Accuracy (train) for ADA: 71.4% 
              precision    recall  f1-score   support

     chinese       0.62      0.46      0.53       257
      indian       0.91      0.87      0.89       224
    japanese       0.69      0.62      0.65       228
      korean       0.63      0.83      0.72       253
        thai       0.75      0.81      0.78       237

    accuracy                           0.71      1199
   macro avg       0.72      0.72      0.71      1199
weighted avg       0.72      0.71      0.71      1199

```

랜덤 포레스트 분류기는 84.6%, AdaBoost는 71.4%로 좋은 결과를 보인다.



✅ [앙상블 분류기](https://scikit-learn.org/stable/modules/svm.html#svm) 알아보기



이 머신러닝 방법은 모델의 품질을 향상시키기 위해 **여러 기본 추정기의 예측을 결합**한다. 이 예제에서는 랜덤 트리와 AdaBoost를 사용했다.

- 평균화 방법인 *[랜덤 포레스트](https://scikit-learn.org/stable/modules/ensemble.html#forest)*는 과대 적합을 피하기 위해 무작위성이 주입된 **결정 트리**의 **포레스트**를 구축한다. 
  - n_estimators 파라미터는 트리의 개수로 설정

- *[AdaBoost](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)*는 분류기를 데이터 집합에 fit시킨 다음 해당 분류기의 복사본을 동일한 데이터 집합에 fit시킨다. 이것은 **잘못 분류된 항목의 가중치**에 초점을 맞추고 다음 분류기가 수정할 fit을 조정한다.





## 4. 요리 추천 Web App 구축

---

요리 데이터셋을 사용하여 분류 모델을 구축하고, *Onnx*의 웹 런타임을 활용하여 저장된 모델을 사용할 수 있는 작은 웹 앱을 만들 것이다.

머신러닝의 가장 유용한 실용적인 사용 방법 중 하나인 추천 시스템을 구축해보자.



### 모델 구축

---

응용 머신러닝 시스템을 구축하는 것은 비즈니스 시스템에 이러한 기술을 활용하는 데 있어 중요한 부분이다. Onnx를 사용하여 웹 응용 프로그램 내에서 모델을 사용할 수 있으므로 필요한 경우 오프라인 에서 모델을 사용할 수 있다.



이 과정에서는 추론을 위한 *기본 JavaScript 기반 시스템* 을 구축할 수 있다. 그러나 먼저 모델을 훈련하고 Onnx에서 사용하도록 변환해야 한다.





### 분류 모델 훈련

---

먼저, 정제된 요리 데이터셋을 사용하여 분류 모델을 훈련한다.

1. 필요한 라이브러리 가져오기

   ```python
   !pip install skl2onnx
   import pandas as pd 
   ```

   Scikit-learn 모델을 Onnx 형식으로 변환하려면 '[skl2onnx](https://onnx.ai/sklearn-onnx/)'가 필요하다.

   

2. 데이터 불러오기

   ```
   data = pd.read_csv(datapath+"cleaned_cuisines.csv")
   data.head() 
   ```

   |      | Unnamed: 0 | cuisine | almond | angelica | anise | anise_seed | apple | apple_brandy | apricot | armagnac | ...  | whiskey | white_bread | hite_wine | whole_grain_wheat_flour | wine | wood | yam  | yeast | yogurt | zucchini |
   | ---- | ---------- | ------- | ------ | -------- | ----- | ---------- | ----- | ------------ | ------- | -------- | ---- | ------- | ----------- | --------- | ----------------------- | ---- | ---- | ---- | ----- | ------ | -------- |
   | 0    | 0          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ...  | 0       | 0           | 0         | 0                       | 0    | 0    | 0    | 0     | 0      | 0        |
   | 1    | 1          | indian  | 1      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ...  | 0       | 0           | 0         | 0                       | 0    | 0    | 0    | 0     | 0      | 0        |
   | 2    | 2          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ...  | 0       | 0           | 0         | 0                       | 0    | 0    | 0    | 0     | 0      | 0        |
   | 3    | 3          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ...  | 0       | 0           | 0         | 0                       | 0    | 0    | 0    | 0     | 0      | 0        |
   | 4    | 4          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ...  | 0       | 0           | 0         | 0                       | 0    | 0    | 0    | 0     | 1      | 0        |

    

3. 불필요한 처음 두 열을 제거하고 데이터를 'X'라고 재명명한다.

   ```
   X = data.iloc[:,2:]
   X.head()
   ```

   |      | almond | angelica | anise | anise_seed | apple | apple_brandy | apricot | armagnac | ...  | whiskey | white_bread | hite_wine | whole_grain_wheat_flour | wine | wood | yam  | yeast | yogurt | zucchini |
   | ---- | ------ | -------- | ----- | ---------- | ----- | ------------ | ------- | -------- | ---- | ------- | ----------- | --------- | ----------------------- | ---- | ---- | ---- | ----- | ------ | -------- |
   | 0    | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ...  | 0       | 0           | 0         | 0                       | 0    | 0    | 0    | 0     | 0      | 0        |
   | 1    | 1      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ...  | 0       | 0           | 0         | 0                       | 0    | 0    | 0    | 0     | 0      | 0        |
   | 2    | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ...  | 0       | 0           | 0         | 0                       | 0    | 0    | 0    | 0     | 0      | 0        |
   | 3    | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ...  | 0       | 0           | 0         | 0                       | 0    | 0    | 0    | 0     | 0      | 0        |
   | 4    | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ...  | 0       | 0           | 0         | 0                       | 0    | 0    | 0    | 0     | 1      | 0        |

    

4. 레이블을 'y'로 저장한다.

   ```
   y = data[['cuisine']]
   y.head()
   ```

   | index | cuisine |
   | ----- | ------- |
   | 0     | indian  |
   | 1     | indian  |
   | 2     | indian  |
   | 3     | indian  |
   | 4     | indian  |





#### 훈련 루틴 시작

---

정확도가 좋은 **'SVC' 라이브러리**를 사용한다.

1. Scikit-learn에서 적절한 라이브러리 가져오기

   ```python
   from sklearn.model_selection import train_test_split
   from sklearn.svm import SVC
   from sklearn.model_selection import cross_val_score
   from sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report
   ```

    

2. 훈련 및 테스트셋 분리

   ```python
   X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)
   ```

    

3. SVC 분류 모델 구축

   ```python
   model = SVC(kernel='linear', C=10, probability=True,random_state=0)
   model.fit(X_train,y_train.values.ravel())
   ```

    

4. `predict()`를 호출하여 모델 테스트

   ```python
   y_pred = model.predict(X_test)
   ```

    

5. 모델 품질 확인을 위해 분류 보고서 출력

   ```python
   print(classification_report(y_test,y_pred))
   ```

   ```output
                 precision    recall  f1-score   support
   
        chinese       0.74      0.67      0.70       258
         indian       0.88      0.87      0.87       234
       japanese       0.74      0.75      0.74       238
         korean       0.83      0.75      0.79       240
           thai       0.72      0.86      0.78       229
   
       accuracy                           0.78      1199
      macro avg       0.78      0.78      0.78      1199
   weighted avg       0.78      0.78      0.78      1199
   ```

   정확도가 좋은 것을 확인할 수 있다.





#### Onxx로 모델 변환

---

반드시 적절한 *Tensor number*로 변환해야 한다. 이 데이터셋에는 380개의 구성 요소가 나열되어 있으므로 `FloatTensorType`에 해당 수를 기록해야 한다.

1. tensor number 380을 사용하여 변환

   ```python
   from skl2onnx import convert_sklearn
   from skl2onnx.common.data_types import FloatTensorType
   
   initial_type = [('float_input', FloatTensorType([None, 380]))]
   options = {id(model): {'nocl': True, 'zipmap': False}}
   ```

    

2. onx를 생상하고 model.onxx 파일에 저장

   ```python
   onx = convert_sklearn(model, initial_types=initial_type, options=options)
   with open("./model.onnx", "wb") as f:
       f.write(onx.SerializeToString())
   ```

   > 변환 스크립트에서 [옵션](https://onnx.ai/sklearn-onnx/parameterized.html)을 전달할 수 있다. 
   >
   > - 이 경우 `'nocl'`을 True로, `'zipmap'`을 False로 전달했다. 
   > - 이 모델은 *분류 모델* 이기 때문에 사전 목록을 생성하는 ZipMap을 제거할 수 있다(필수 아님). 
   > - `nocl`은 모델에 포함된 클래스 정보를 참조하지 않는다.
   >   -  `nocl`을 'True'로 설정하여 모델 크기를 줄인다.

전체 노트북을 실행하면 Onnx 모델이 구축되어 폴더에 저장된다.





### 모델 보기

---

Onnx 모델은 Visual Studio 코드에서 잘 보이지 않지만, 많은 연구자들이 **모델을 시각화**하기 위해 사용하는 매우 좋은 무료 소프트웨어인  *Netron*은 모델을 보는 데 유용한 도구이다.

- [Netron](https://github.com/lutzroeder/Netron)을 다운로드하고 model.onnx 파일을 열면 380개의 입력과 분류기가 나열된 단순한 모델을 시각화할 수 있다.

<img src="https://github.com/codingalzi/ML-For-Beginners/raw/main/4-Classification/4-Applied/images/netron.png" alt="Netron visual" style="zoom: 80%;" />





이제 웹 앱에서 이 깔끔한 모델을 사용할 준비가 되었다. 냉장고 안을 볼 때 유용하게 사용할 수 있는 앱을 만들고, 모델에 의해 결정되는 대로, 주어진 요리를 요리하기 위해 어떤 *식재료의 조합* 을 사용할 수 있는지 알아보자.





### 추천 web application 구축

---

웹 앱에서 직접 모델을 사용할 수 있다. 또한 이 아키텍처를 통해 필요한 경우 로컬 및 오프라인에서도 실행할 수 있다. `model.onnx` 파일을 저장한 폴더와 동일한 폴더에 `index.html` 파일을 생성하는 것부터 시작하자.

1. *index.html* 파일에서 다음 markup을 추가한다.

   ```html
   <!DOCTYPE html>
   <html>
       <header>
           <title>Cuisine Matcher</title>
       </header>
       <body>
           ...
       </body>
   </html>
   ```

    

2. 이제 `body` 태그 내에서 작업하면서 일부 식재료를 반영하는 체크박스 목록을 표시하기 위해 markup을 추가한다.

   ```html
   <h1>Check your refrigerator. What can you create?</h1>
           <div id="wrapper">
               <div class="boxCont">
                   <input type="checkbox" value="4" class="checkbox">
                   <label>apple</label>
               </div>
           
               <div class="boxCont">
                   <input type="checkbox" value="247" class="checkbox">
                   <label>pear</label>
               </div>
           
               <div class="boxCont">
                   <input type="checkbox" value="77" class="checkbox">
                   <label>cherry</label>
               </div>
   
               <div class="boxCont">
                   <input type="checkbox" value="126" class="checkbox">
                   <label>fenugreek</label>
               </div>
   
               <div class="boxCont">
                   <input type="checkbox" value="302" class="checkbox">
                   <label>sake</label>
               </div>
   
               <div class="boxCont">
                   <input type="checkbox" value="327" class="checkbox">
                   <label>soy sauce</label>
               </div>
   
               <div class="boxCont">
                   <input type="checkbox" value="112" class="checkbox">
                   <label>cumin</label>
               </div>
           </div>
           <div style="padding-top:10px">
               <button onClick="startInference()">What kind of cuisine can you make?</button>
           </div> 
   ```

   - 각 체크박스에는 값이 지정된다. 이는 데이터셋에 따라 식재료가 발견되는 인덱스를 반영한다. 
     - 예를 들어, 이 알파벳 목록에서 Apple은 다섯 번째 열을 차지하기 때문에, 0에서 숫자를 세기 시작할 때 값은 '4'이다. 
   - [ingredients spreadsheet](https://github.com/codingalzi/ML-For-Beginners/blob/main/4-Classification/data/ingredient_indexes.csv)를 참조하여 특정 식재료의 색인을 찾을 수 있다.
   - *index.html* 파일에서 작업하고, 최종 종료 `</div>` 뒤에 모델이 호출되는 스크립트 블록을 추가한다.

   

3. 먼저, [Onnx Runtime](https://www.onnxruntime.ai/)을 가져온다.

   ```html
   <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web@1.9.0/dist/ort.min.js"></script> 
   ```

   >  *Onnx* 런타임은 최적화 및 사용할 API를 포함한 광범위한 하드웨어 플랫폼에서 Onnx 모델을 실행할 수 있도록 하는 데 사용된다.

   

4. 런타임이 설치되면 다음과 같이 호출할 수 있다.

   ```html
   <script>
       const ingredients = Array(380).fill(0);
       
       const checks = [...document.querySelectorAll('.checkbox')];
       
       checks.forEach(check => {
           check.addEventListener('change', function() {
               // toggle the state of the ingredient
               // based on the checkbox's value (1 or 0)
               ingredients[check.value] = check.checked ? 1 : 0;
           });
       });
   
       function testCheckboxes() {
           // validate if at least one checkbox is checked
           return checks.some(check => check.checked);
       }
   
       async function startInference() {
   
           let atLeastOneChecked = testCheckboxes()
   
           if (!atLeastOneChecked) {
               alert('Please select at least one ingredient.');
               return;
           }
           try {
               // create a new session and load the model.
               
               const session = await ort.InferenceSession.create('./model.onnx');
   
               const input = new ort.Tensor(new Float32Array(ingredients), [1, 380]);
               const feeds = { float_input: input };
   
               // feed inputs and run
               const results = await session.run(feeds);
   
               // read from results
               alert('You can enjoy ' + results.label.data[0] + ' cuisine today!')
   
           } catch (e) {
               console.log(`failed to inference ONNX model`);
               console.error(e);
           }
       }
              
   </script>
   ```

   이 코드에는 다음과 같은 몇 가지 일이 발생한다.

   1. *식재료 체크박스가 선택되었는지 여부* 에 따라 380개의 가능한 값(1 또는 0)을 설정하고 추론을 위한 모델로 전송

   2. 체크박스 배열을 생성 및 앱이 시작될 때 호출되는 `init` 함수로 확인되었는지 확인하는 방법  결정

      - 체크박스를 선택하면 선택한 식재료를 반영하도록 `ingredients` 배열이 변경된다.

   3. *체크박스가 선택되었는지 여부를 확인* 하는 `testCheckboxes` 함수 생성

   4. 버튼을 누르면 `startInference` 함수를 사용하고, 체크박스를 선택하면 추론 시작

   5. 추론 루틴에는 다음이 포함된다.
   
      ⅰ. 모델의 비동기 로드 설정
      ⅱ. 모델에 보낼 Tensor 구조 생성
      ⅲ. 모델을 훈련할 때 만든 `float_input` 입력을 반영하는 'feeds' 생성(Netron을 사용하여 해당 이름을 확인할 수 있음)
      ⅳ. 이러한 'feeds'를 모델에 보내고 응답을 기다리기





### 앱 테스트

---

1. *index.html* 파일이 있는 폴더안의 Visual Studio Code에서 터미널 세션을 연다. 
2. [http-server](https://www.npmjs.com/package/http-server) 가 global으로 설치되어 있는지 확인하고 프롬프트에 `http-server`를 입력한다. 
3. 로컬 호스트가 열리면 웹 앱을 볼 수 있다. 다양한 식재료에 따라 어떤 요리를 추천하는지 확인한다.

<img src="https://github.com/codingalzi/ML-For-Beginners/raw/main/4-Classification/4-Applied/images/web-app.png" alt="ingredient web app" style="zoom:50%;" />

> 출처: https://github.com/codingalzi/ML-For-Beginners/tree/main/4-Classification
